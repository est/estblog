<!DOCTYPE html>
<html>
  <head>
    <title>åäººçš„åˆä¸€å¤§â€œå‘æ˜â€ï¼šå¿½ç•¥robots.txtç›‘æ§è®ºå› - â„°ğ’®ğ’¯ ğŸ’« ğ”¹ã’</title>

    <meta charset="utf-8" />
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <link href="https://feeds.feedburner.com/initiative" type="application/rss+xml" rel="alternate" title="â„°ğ’®ğ’¯ ğŸ’« ğ”¹ã’ RSS Feed" />

    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="åäººçš„åˆä¸€å¤§â€œå‘æ˜â€ï¼šå¿½ç•¥robots.txtç›‘æ§è®ºå›" />

      <link rel="canonical" href="https://blog.est.im/archives/2011" />

    <link rel="stylesheet" href="/theme/css/normalize.min.css" />
    <link rel="stylesheet" href="/theme/css/style.css" />


    <style>
      body {
        background: #ecedef url("/theme/img/ignasi_pattern_s.png") repeat;
      }
    </style>
  </head>
  <body class="single-body">
<nav class="nav-bar side-padding">
  <h1 class="nav-header"><a href="/" class="nav-text">
    â„°ğ’®ğ’¯ ğŸ’« ğ”¹ã’
  </a></h1>
  <div class="hamburger-menu">
    <button onclick="hamburgerMenuPressed.call(this)" aria-haspopup="true" aria-expanded="false" aria-controls="menu" aria-label="Menu">
      <span></span>
      <span></span>
    </button>
    <ul id="menu" class="hamburger-menu-overlay">
      <li><a href="/" class="hamburger-menu-overlay-link">Home</a></li>
      <li><a href="https://feeds.feedburner.com/initiative" class="hamburger-menu-overlay-link">RSS</a></li>
      <li><a href="/about" class="hamburger-menu-overlay-link">About</a></li>

      <li><a href="/category/archive" class="hamburger-menu-overlay-link active">archive</a></li>
      <li><a href="/category/stderr" class="hamburger-menu-overlay-link">stderr</a></li>
      <li><a href="/category/stdin" class="hamburger-menu-overlay-link">stdin</a></li>
      <li><a href="/category/stdout" class="hamburger-menu-overlay-link">stdout</a></li>

    </ul>
  </div>
</nav>    <main class="content side-text-padding">


<article class="post">
  <header class="post-header">
    <h2 class="post-title">åäººçš„åˆä¸€å¤§â€œå‘æ˜â€ï¼šå¿½ç•¥robots.txtç›‘æ§è®ºå›</h2>
    <p class="post-date">Posted <time datetime="2010-04-14T12:52:19+08:00">2010-04-14</time> | <span>archive</span></p>
  </header>

  
  <p>è™½ç„¶å¿½ç•¥robots.txtä¼ªè£…æµè§ˆå™¨è¡Œä¸ºæ¥æ‰’ç«™ç›‘æ§è¨€è®ºï¼Œåœ¨å…²æœæ—©å°±ä¸æ˜¯ä»€ä¹ˆç§˜å¯†ï¼Œä½†æ˜¯ä¸€ä¸ªæ­£å„¿å…«ç»çš„æ¥è‡ªU of Arizonaçš„<a href="http://ai.arizona.edu/hchen/">åäººPh.D</a>å±…ç„¶è¿˜æŠŠè¿™ä¸œè¥¿å†™æˆpaperç»™â€œå‘æ˜â€å‡ºæ¥ï¼Œæˆ‘è¿˜æ˜¯æ„Ÿåˆ°æ¯”è¾ƒéœ‡ç²¾çš„ã€‚</p>

<p>paperåœ°å€<br>
<a href="http://dx.doi.org/10.1002/asi.21323">http://dx.doi.org/10.1002/asi.21323</a></p>

<p>paperæ¦‚è§ˆ<br>
<a href="https://docs.google.com/viewer?url=http://ai.arizona.edu/research/terror/forum_poster.pdf">https://docs.google.com/viewer?url=http://ai.arizona.edu/research/terror/forum_poster.pdf</a></p>

<p>paper abstract</p>

<blockquote>
  <p>The unprecedented growth of the Internet has given rise to the Dark Web, the problematic facet of the Web associated with cybercrime, hate, and extremism. Despite the need for tools to collect and analyze Dark Web forums, the covert nature of this part of the Internet makes traditional Web crawling techniques insufficient for capturing such content. In this study, we propose a novel crawling system designed to collect Dark Web forum content. The system uses a human-assisted accessibility approach to gain access to Dark Web forums. Several URL ordering features and techniques enable efficient extraction of forum postings. The system also includes an incremental crawler coupled with a recall-improvement mechanism intended to facilitate enhanced retrieval and updating of collected content. Experiments conducted to evaluate the effectiveness of the human-assisted accessibility approach and the recall-improvement-based, incremental-update procedure yielded favorable results. The human-assisted approach significantly improved access to Dark Web forums while the incremental crawler with recall improvement also outperformed standard periodic- and incremental-update approaches. Using the system, we were able to collect over 100 Dark Web forums from three regions. A case study encompassing link and content analysis of collected forums was used to illustrate the value and importance of gathering and analyzing content from such online communities.</p>
</blockquote>

<p>robots.txtå’Œç”¨æˆ·æ³¨å†Œæœ¬æ¥å°±æ˜¯é˜²å›å­ä¸é˜²å°äººçš„ä¸œè¥¿ã€‚æ­£è§„å­¦æœ¯ç ”ç©¶è¿™æ ·ä¹±æï¼Œæˆ‘åªèƒ½è¯´ä¸¤ä¸ªå­—ï¼šæ— è€»ã€‚é‡åˆ°è¿™ç§æµæ°“æœºæ„çš„æµæ°“çˆ¬è™«ï¼Œåªæœ‰æä¸€ä¸ªç±»ä¼¼nmapåŸºäºè¡Œä¸ºï¼Œè€Œä¸æ˜¯åŸºäºç‰¹å¾çš„é‰´åˆ«ç³»ç»Ÿæ‰èƒ½å¯¹ä»˜ã€‚</p>

<p>from <a href="http://www.newscientist.com/article/mg20627555.700-web-spy-software-hacks-into-secretive-online-forums.html">newscientist</a> via <a href="http://www.reddit.com/r/netsec/comments/bqqdw/government_invents_search_spider_that_ignores/">reddit</a></p>

</article>


<div class="comments">
  <h2>Comments</h2>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname = 'estblog';
    var disqus_identifier = 'archives/2011';
    var disqus_url = location.href;
    (function() {
    var dsq = document.createElement('script'); 

    dsq.type = 'text/javascript';
    dsq.defer = true;
    dsq.src = '//estblog.disqus.com/embed.js';
    (document.getElementsByTagName('body')[0] || document.getElementsByTagName('head')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the comments.</noscript>
</div>    </main>
<script src="/theme/js/core.js"></script>

  </body>

</html>