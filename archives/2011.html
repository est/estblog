<!DOCTYPE html>
<html>
  <head>
    <title>华人的又一大“发明”：忽略robots.txt监控论坛 - est の 输入输出</title>

    <meta charset="utf-8" />
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <link href="https://feeds.feedburner.com/initiative" type="application/rss+xml" rel="alternate" title="est の 输入输出 RSS Feed" />

    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="华人的又一大“发明”：忽略robots.txt监控论坛" />

    <link rel="canonical" href="https://blog.est.im/archives/2011" />
    <meta property="og:url" content="/archives/2011">
    <meta property="og:type" content="article">
    <meta property="og:title" content="华人的又一大“发明”：忽略robots.txt监控论坛">

    <link rel="stylesheet" href="/theme/css/normalize.min.css" />
    <link rel="stylesheet" href="/theme/css/style.css" />


    <style>
      body {
        background: #ecedef url("/theme/img/ignasi_pattern_s.png") repeat;
      }
    </style>
  </head>
  <body class="single-body">
<nav class="nav-bar side-padding">
  <h1 class="nav-header"><a href="/" class="nav-text">
    est の 输入输出
  </a></h1>
  <div class="hamburger-menu">
    <button onclick="hamburgerMenuPressed.call(this)" aria-haspopup="true" aria-expanded="false" aria-controls="menu" aria-label="Menu">
      <span></span>
      <span></span>
    </button>
    <ul id="menu" class="hamburger-menu-overlay">
      <li><a href="/" class="hamburger-menu-overlay-link">Home</a></li>
      <li><a href="https://feeds.feedburner.com/initiative" class="hamburger-menu-overlay-link">RSS</a></li>
      <li><a href="/about" class="hamburger-menu-overlay-link">About</a></li>

      <li><a href="/category/archive" class="hamburger-menu-overlay-link active">archive</a></li>
      <li><a href="/category/stderr" class="hamburger-menu-overlay-link">stderr</a></li>
      <li><a href="/category/stdin" class="hamburger-menu-overlay-link">stdin</a></li>
      <li><a href="/category/stdout" class="hamburger-menu-overlay-link">stdout</a></li>

    </ul>
  </div>
</nav>    <main class="content side-text-padding">


<article class="post">
  <header class="post-header">
    <h2 class="post-title">华人的又一大“发明”：忽略robots.txt监控论坛</h2>
    <p class="post-date">Posted <time datetime="2010-04-14T12:52:19+08:00">2010-04-14</time> | <span>archive</span></p>
  </header>

  
  <p>虽然忽略robots.txt伪装浏览器行为来扒站监控言论，在兲朝早就不是什么秘密，但是一个正儿八经的来自U of Arizona的<a href="http://ai.arizona.edu/hchen/">华人Ph.D</a>居然还把这东西写成paper给“发明”出来，我还是感到比较震精的。</p>

<p>paper地址<br>
<a href="http://dx.doi.org/10.1002/asi.21323">http://dx.doi.org/10.1002/asi.21323</a></p>

<p>paper概览<br>
<a href="https://docs.google.com/viewer?url=http://ai.arizona.edu/research/terror/forum_poster.pdf">https://docs.google.com/viewer?url=http://ai.arizona.edu/research/terror/forum_poster.pdf</a></p>

<p>paper abstract</p>

<blockquote>
  <p>The unprecedented growth of the Internet has given rise to the Dark Web, the problematic facet of the Web associated with cybercrime, hate, and extremism. Despite the need for tools to collect and analyze Dark Web forums, the covert nature of this part of the Internet makes traditional Web crawling techniques insufficient for capturing such content. In this study, we propose a novel crawling system designed to collect Dark Web forum content. The system uses a human-assisted accessibility approach to gain access to Dark Web forums. Several URL ordering features and techniques enable efficient extraction of forum postings. The system also includes an incremental crawler coupled with a recall-improvement mechanism intended to facilitate enhanced retrieval and updating of collected content. Experiments conducted to evaluate the effectiveness of the human-assisted accessibility approach and the recall-improvement-based, incremental-update procedure yielded favorable results. The human-assisted approach significantly improved access to Dark Web forums while the incremental crawler with recall improvement also outperformed standard periodic- and incremental-update approaches. Using the system, we were able to collect over 100 Dark Web forums from three regions. A case study encompassing link and content analysis of collected forums was used to illustrate the value and importance of gathering and analyzing content from such online communities.</p>
</blockquote>

<p>robots.txt和用户注册本来就是防君子不防小人的东西。正规学术研究这样乱搞，我只能说两个字：无耻。遇到这种流氓机构的流氓爬虫，只有搞一个类似nmap基于行为，而不是基于特征的鉴别系统才能对付。</p>

<p>from <a href="http://www.newscientist.com/article/mg20627555.700-web-spy-software-hacks-into-secretive-online-forums.html">newscientist</a> via <a href="http://www.reddit.com/r/netsec/comments/bqqdw/government_invents_search_spider_that_ignores/">reddit</a></p>

</article>


<div class="comments">
  <h2>Comments</h2>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname = 'estblog';
    var disqus_identifier = 'archives/2011';
    var disqus_url = location.href;
    (function() {
    var dsq = document.createElement('script'); 

    dsq.type = 'text/javascript';
    dsq.defer = true;
    dsq.src = '//estblog.disqus.com/embed.js';
    (document.getElementsByTagName('body')[0] || document.getElementsByTagName('head')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the comments.</noscript>
</div>    </main>
<script src="/theme/js/core.js"></script>

  </body>

</html>